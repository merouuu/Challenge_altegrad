# Guide d'entra√Ænement optimis√© - Corrections critiques appliqu√©es

## üî¥ Corrections critiques appliqu√©es

### 1. **FIX MAJEUR: D√©salignement Dataset/Sampler** ‚úÖ
**Probl√®me:** Le HardNegativeSampler construisait les embeddings dans l'ordre **lexicographique des IDs**, mais le dataset √©tait dans l'ordre du **pickle**. R√©sultat : on faisait du batching sur de mauvaises mol√©cules !

**Solution:** Le sampler prend maintenant le dataset en param√®tre et utilise son ordre r√©el.

```python
# AVANT (bug)
sampler = HardNegativeSampler(train_emb, batch_size=32)

# APR√àS (correct)
sampler = HardNegativeSampler(train_emb, batch_size=32, dataset=train_ds)
```

### 2. **AttentionPooling optimis√©e** ‚úÖ
- Remplac√© calcul manuel du softmax par `torch_geometric.utils.softmax` (plus stable num√©riquement)
- √âlimin√© calcul redondant de `scatter(...).exp()`

### 3. **Graph Augmentation ajout√©e** ‚úÖ
- Edge dropout : supprime 10% des edges al√©atoirement
- Node feature dropout : masque 5% des features
- Force le mod√®le √† apprendre des invariances

### 4. **Temperature Schedule** ‚úÖ
- D√©marre √† 0.1 (softmax plus diffus, moins aiguis√©)
- D√©cro√Æt lin√©airement jusqu'√† 0.07 (plus s√©lectif en fin)
- √âvite sur-sp√©cialisation trop t√¥t

### 5. **BLEU-4 + BERTScore Evaluation** ‚úÖ
- Mesure des m√©triques que Kaggle utilise r√©ellement
- Permet de piloter en fonction du score final, pas juste MRR

---

## üöÄ Commandes optimales par strat√©gie

### **Strat√©gie 1 : D√©marrage complet (recommand√©)**

```bash
python train_gt_contrast.py \
  --env colab \
  --epochs 100 \
  --batch_size 128 \
  --lr 0.0003 \
  --temp 0.07 \
  --hard_negatives \
  --hard_ratio 0.5 \
  --hardness_k 100 \
  --curriculum_epoch 5 \
  --use_augmentation \
  --temp_schedule
```

**Phases:**
- Epochs 1-5 : Random sampling + warm-up
- Epochs 6-100 : Hard negative mining + temperature decay

**Taux de succ√®s attendu:** MRR 0.35-0.45, BLEU-4 0.15-0.20

### **Strat√©gie 2 : Curriculum + √âvaluation texte**

```bash
python train_gt_contrast.py \
  --env colab \
  --epochs 100 \
  --batch_size 128 \
  --lr 0.0003 \
  --temp 0.07 \
  --hard_negatives \
  --hard_ratio 0.5 \
  --hardness_k 100 \
  --curriculum_epoch 5 \
  --use_augmentation \
  --temp_schedule \
  --eval_bleu_bert
```

**Avantage:** Voit les scores BLEU-4 + BERTScore r√©els (ce que Kaggle mesure)

### **Strat√©gie 3 : Hard mode agressif (fine-tuning)**

```bash
python train_gt_contrast.py \
  --env colab \
  --epochs 50 \
  --batch_size 128 \
  --lr 0.0001 \
  --temp 0.07 \
  --hard_negatives \
  --hard_ratio 0.7 \
  --hardness_k 50 \
  --curriculum_epoch 0 \
  --use_augmentation \
  --temp_schedule \
  --resume_from GT_Contrast/contrastive_model.pt
```

**Quand l'utiliser:** Vous reprenez un mod√®le d√©j√† entra√Æn√© (epochs 5+)

---

## üìä Hyperparam√®tres cl√©s et recommandations

### Hard Ratio (apr√®s curriculum)

| Valeur | Profil | Cas d'usage |
|--------|--------|-----------|
| 0.3 | Conservateur | D√©marrage instable, small batch |
| **0.5** | **√âquilibr√© (üëà recommand√©)** | **Tous les cas** |
| 0.7 | Agressif | Fine-tuning |
| 0.8+ | Hardcore | Risque de divergence |

### Curriculum Epoch

| Valeur | Effet | Cas d'usage |
|--------|-------|-----------|
| 0 | Hard d√®s le d√©but | Mod√®le d√©j√† pr√©tra√Æn√© |
| **3-5** | **Recommand√©** | **D√©marrage cold** |
| 10+ | Trop tard | Mod√®le a d√©j√† converg√© |

### Temperature Schedule

**Activ√© (recommand√©):**
- D√©marre 0.1 ‚Üí Finit 0.07
- √âvite sur-aiguisage trop t√¥t
- +2-3% MRR souvent

**D√©sactiv√©:**
- Temp√©rature fixe √† 0.07
- Peut causer oscillation

---

## üéØ R√©sultats attendus

### Avec les corrections

| M√©trique | Sans hard neg | Avec curriculum | Avec augmentation |
|----------|--------------|-----------------|-------------------|
| **MRR** | 0.30-0.35 | 0.40-0.50 | 0.42-0.52 |
| **R@1** | 0.15-0.20 | 0.25-0.35 | 0.27-0.37 |
| **R@5** | 0.45-0.55 | 0.60-0.70 | 0.62-0.72 |
| **BLEU-4** | 0.10-0.13 | 0.15-0.18 | 0.16-0.20 |

---

## ‚ö†Ô∏è Points critiques

### 1. Dataset alignment ‚úÖ
- Le sampler re√ßoit `dataset=train_ds`
- Les neighbors sont dans l'ordre r√©el du dataset
- **V√©rifier:** Les logs affichent `‚úÖ Utilisation de l'ordre r√©el du dataset`

### 2. √âvaluation texte
- Si `--eval_bleu_bert` : n√©cessite `nltk` et `bert_score`
- Installation: `pip install nltk bert-score`
- Sinon : juste MRR/R@k (toujours valide)

### 3. Temp√©rature schedule
- Si activ√©: temp√©rature adaptative par epoch
- Si d√©sactiv√©: fixe √† `--temp` (0.07)
- B√©n√©fice: ~2-3% de gain MRR

### 4. Augmentation
- Edge dropout: 10% des edges
- Node dropout: 5% des features
- Ajoute robustesse, l√©g√®rement ralentit chaque epoch

---

## üìà Monitoring pendant l'entra√Ænement

### Signes positifs ‚úÖ

```
Epoch 01/100 | Train Loss: 3.5152 | Val: MRR: 0.1124
Epoch 02/100 | Train Loss: 3.1656 | Val: MRR: 0.1467  ‚Üê mont√©e r√©guli√®re
Epoch 05/100 | Train Loss: 2.9624 | Val: MRR: 0.2012

üéì CURRICULUM SWITCH: Activation du Hard Negative Mining √† l'epoch 6
Epoch 06/100 | Train Loss: 3.0891 | Val: MRR: 0.2045  ‚Üê petit pic de loss, MRR continue
Epoch 07/100 | Train Loss: 2.8945 | Val: MRR: 0.2234  ‚Üê reprend sa mont√©e
```

### Signes probl√©matiques ‚ö†Ô∏è

```
Epoch 10/100 | Train Loss: 2.8000 | Val: MRR: 0.3000
Epoch 11/100 | Train Loss: 2.7500 | Val: MRR: 0.3001  ‚Üê stagnation
Epoch 12/100 | Train Loss: 2.8200 | Val: MRR: 0.2987  ‚Üê baisse (divergence possible)
```

**Actions:**
1. R√©duire `--hard_ratio` (0.5 ‚Üí 0.3)
2. R√©duire `--lr` (0.0003 ‚Üí 0.0001)
3. Repousser curriculum (5 ‚Üí 8)

---

## üîó Recommandations finales

### Pour soumettre √† Kaggle

1. **Entra√Æner avec la strat√©gie 1** (curriculum + augmentation)
2. **Valider avec BLEU-4 + BERTScore** (strategy 2)
3. **Si plateauing**: passer en "mode agressif" (strategy 3)
4. **Toujours sauvegarder** `GT_Contrast/contrastive_model.pt`

### Ordre de priorit√© si RAM limite

1. Garder `--hard_negatives` + curriculum (gain majeur)
2. Augmenter batch_size √† 64+ (contrastive loss aime √ßa)
3. R√©duire `--hidden` si OOM (128 ‚Üí 96)
4. `--use_augmentation` optionnel (gain mod√©r√©)

---

## üìù Notes techniques

- **HardNegativeSampler** : O(N*K) precomp, puis O(batch) per epoch
- **Temperature schedule** : d√©croissance lin√©aire `init_temp - (init_temp - final)*progress`
- **Softmax PyG** : stable num√©riquement m√™me pour gros batches
- **BLEU-4** : requires tokenization (nltk.punkt)
- **BERTScore** : requires pretrained BERT (auto-download)

Bon chance ! üöÄ
